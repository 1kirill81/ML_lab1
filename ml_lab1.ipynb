{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3AVU0lmkMdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c615e9-286c-4e9e-defc-7e6fffb615e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-huggingface langchain-core transformers accelerate pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yqpk2K9lRLI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_from_url(url):\n",
        "    print(f\"Загрузка данных из {url}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(url)\n",
        "        print(f\"Успешно загружено {len(df)} строк\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка загрузки: {e}\")\n",
        "        return None\n",
        "\n",
        "DEV_DATA_URL = \"https://raw.githubusercontent.com/1kirill81/ML_lab1/refs/heads/main/LR1_dev_mini.csv\"\n",
        "ANSWER_DATA_URL = \"https://raw.githubusercontent.com/1kirill81/ML_lab1/refs/heads/main/LR1_dev_mini_answers.csv\"\n",
        "\n",
        "\n",
        "df_questions = load_data_from_url(DEV_DATA_URL)\n",
        "df_answers = load_data_from_url(ANSWER_DATA_URL)\n",
        "\n",
        "\n",
        "def parse_options(options_str):\n",
        "    if pd.isna(options_str) or not isinstance(options_str, str):\n",
        "        return []\n",
        "\n",
        "    text = str(options_str).strip()\n",
        "\n",
        "\n",
        "    if text.startswith('[') and text.endswith(']'):\n",
        "        text = text[1:-1].strip()\n",
        "\n",
        "\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "\n",
        "    pattern = r'[\\'\"]([^\\'\"]*?)[\\'\"]'\n",
        "    matches = re.findall(pattern, text)\n",
        "\n",
        "    if matches:\n",
        "        return matches\n",
        "\n",
        "\n",
        "    parts = re.split(r'\\s{2,}', text)\n",
        "    return [p.strip(' \\'\"') for p in parts if p.strip()]\n",
        "\n",
        "\n",
        "# Задаем списку ответов читаемы вид\n",
        "def format_options_for_display(options_list, max_length=100):\n",
        "    if not options_list:\n",
        "        return \"Нет вариантов\"\n",
        "\n",
        "    formatted = []\n",
        "    for i, opt in enumerate(options_list):\n",
        "        # Обрезаем длинные варианты\n",
        "        if len(opt) > max_length:\n",
        "            opt = opt[:max_length-3] + \"...\"\n",
        "        formatted.append(f\"{i}. {opt}\")\n",
        "\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "\n",
        "# Функция для вытягивания номера ответа\n",
        "def extract_answer_from_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return -1\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    # Ищем цифры в тексте\n",
        "    numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "\n",
        "    if not numbers:\n",
        "        return -1\n",
        "\n",
        "    # Берем последнее число (обычно это финальный ответ)\n",
        "    try:\n",
        "        answer = int(numbers[-1])\n",
        "        if 0 <= answer <= 99:  # Проверяем разумный диапазон\n",
        "            return answer\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return -1\n",
        "\n",
        "\n",
        "# Функция загрузки модельки и создания промта\n",
        "def init_model_and_promt(model_name=\"microsoft/phi-2\"):\n",
        "  print(f\"\\n\\nЗагрузка модели: {model_name}\\n\\n\")\n",
        "\n",
        "  model = model_name\n",
        "\n",
        "  hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    device_map='auto',\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.1\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "  prompt_template = PromptTemplate(\n",
        "            input_variables=[\"category\", \"question\", \"options\"],\n",
        "            template=\"\"\"Ответь на вопрос, но перед этим обдумай кажды вариант ответа из предложенных\n",
        "Категория: {category}\n",
        "Вопрос: {question}\n",
        "Варианты: {options}\n",
        "Правильный вариант: \"\"\"\n",
        "          )\n",
        "\n",
        "  return llm, prompt_template\n",
        "\n",
        "\n",
        "def procces_question(llm, prompt_template, category, question, options_str, question_num=0):\n",
        "  try:\n",
        "    options_list = parse_options(options_str)\n",
        "    foptions = format_options_for_display(options_list)\n",
        "    prompt = prompt_template.format(\n",
        "        category=category,\n",
        "        question=question,\n",
        "        options=foptions\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    promt_template2 = '''Тебе будет предоставлен ход решения задачи. Необходимо извлечь финальный ответ и написать только его числом.\n",
        "Решение:{decision}\n",
        "\n",
        "Ответ:'''\n",
        "\n",
        "    promt2 = promt_template2.format(\n",
        "        decision=response\n",
        "    )\n",
        "\n",
        "    response2 = llm.invoke(promt2)\n",
        "\n",
        "    answer_number = extract_answer_from_text(response2)\n",
        "\n",
        "    return answer_number\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "        print(f\"Ошибка в вопросе {question_num}: {e}\")\n",
        "        return -1\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "  llm, prompt_template = init_model_and_promt()\n",
        "  row = df_questions.iloc[3]\n",
        "  answer = procces_question(\n",
        "                llm=llm,\n",
        "                prompt_template=prompt_template,\n",
        "                category=row.get(\"category\"),\n",
        "                question=row.get(\"question\"),\n",
        "                options_str=str(row.get(\"options\")),\n",
        "                question_num=0\n",
        "            )\n",
        "  print(f\"Вопрос {3}: {answer}\")\n",
        "\n",
        "  # all_ans = []\n",
        "\n",
        "  # print(\"Запуск обратоки. Кол-во вопросов: \", len(df_questions))\n",
        "\n",
        "  # for i in range(len(df_questions)):\n",
        "  #   print(f\"Обработка вопроса {i}\")\n",
        "  #   row = df_questions.iloc[i]\n",
        "  #   answer = procces_question(\n",
        "  #               llm=llm,\n",
        "  #               prompt_template=prompt_template,\n",
        "  #               category=row.get(\"category\"),\n",
        "  #               question=row.get(\"question\"),\n",
        "  #               options_str=str(row.get(\"options\")),\n",
        "  #               question_num=0\n",
        "  #           )\n",
        "  #   all_ans.append(answer[1])\n",
        "  #   print(\"=\"*30)\n",
        "  #   print(f\"Вопрос {i}: {answer[1]}\\nПолный ответ: \\n{answer[0]}\")\n",
        "  #   print(\"=\"*30)\n",
        "\n",
        "  # print(all_ans)\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "4Fr0iIGqnMuM",
        "outputId": "12951216-b71f-4716-9c9d-16ff99bda73a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка данных из https://raw.githubusercontent.com/1kirill81/ML_lab1/refs/heads/main/LR1_dev_mini.csv...\n",
            "Ошибка загрузки: name 'pd' is not defined\n",
            "Загрузка данных из https://raw.githubusercontent.com/1kirill81/ML_lab1/refs/heads/main/LR1_dev_mini_answers.csv...\n",
            "Ошибка загрузки: name 'pd' is not defined\n",
            "\n",
            "\n",
            "Загрузка модели: microsoft/phi-2\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-802642127.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-802642127.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m   \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model_and_promt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_questions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m   answer = procces_question(\n",
            "\u001b[0;32m/tmp/ipython-input-802642127.py\u001b[0m in \u001b[0;36minit_model_and_promt\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m   hf_pipeline = pipeline(\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HnCdChgoFcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_s9-SOyLZnIS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}